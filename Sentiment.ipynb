{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path # File I/O\n",
    "import re # Useful for cleaning data\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer # I'll try both\n",
    "from sklearn.svm import SVC # I think that using a SVM should be best for this problem\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from textblob import TextBlob # To correct spelling errors throughout the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\_Sentiment\\Datasets\\ISEAR_0\\isear_databank\\ISEAR_emotion.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Set up filepath to read in sentiment data\n",
    "basepath = os.path.dirname('__file__')\n",
    "filepath = os.path.abspath(os.path.join(basepath, \n",
    "                                       'Datasets',\n",
    "                                       'ISEAR_0', \n",
    "                                       'isear_databank',\n",
    "                                       'ISEAR_emotion.xlsx'))\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CITY</th>\n",
       "      <th>COUN</th>\n",
       "      <th>SUBJ</th>\n",
       "      <th>SEX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>RELI</th>\n",
       "      <th>PRAC</th>\n",
       "      <th>FOCC</th>\n",
       "      <th>MOCC</th>\n",
       "      <th>...</th>\n",
       "      <th>SELF</th>\n",
       "      <th>RELA</th>\n",
       "      <th>VERBAL</th>\n",
       "      <th>NEUTRO</th>\n",
       "      <th>Field1</th>\n",
       "      <th>Field3</th>\n",
       "      <th>Field2</th>\n",
       "      <th>MYKEY</th>\n",
       "      <th>SIT</th>\n",
       "      <th>STATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>joy</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>110011</td>\n",
       "      <td>During the period of falling in love, each tim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fear</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>110012</td>\n",
       "      <td>When I was involved in a traffic accident.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>anger</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>110013</td>\n",
       "      <td>When I was driving home after  several days of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>110014</td>\n",
       "      <td>When I lost the person who meant the most to me.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>disgust</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>110015</td>\n",
       "      <td>The time I knocked a deer down - the sight of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows ร 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  CITY  COUN  SUBJ  SEX  AGE  RELI  PRAC  FOCC  MOCC  ...    SELF  \\\n",
       "0  11001     1     1     1    1   33     1     2     6     1  ...       3   \n",
       "1  11001     1     1     1    1   33     1     2     6     1  ...       2   \n",
       "2  11001     1     1     1    1   33     1     2     6     1  ...       2   \n",
       "3  11001     1     1     1    1   33     1     2     6     1  ...       1   \n",
       "4  11001     1     1     1    1   33     1     2     6     1  ...       0   \n",
       "\n",
       "   RELA  VERBAL  NEUTRO   Field1  Field3  Field2   MYKEY  \\\n",
       "0     3       2       0      joy       4       3  110011   \n",
       "1     2       0       0     fear       3       2  110012   \n",
       "2     1       0       0    anger       1       3  110013   \n",
       "3     1       0       2  sadness       4       4  110014   \n",
       "4     2       0       0  disgust       4       4  110015   \n",
       "\n",
       "                                                 SIT  STATE  \n",
       "0  During the period of falling in love, each tim...      1  \n",
       "1         When I was involved in a traffic accident.      1  \n",
       "2  When I was driving home after  several days of...      1  \n",
       "3   When I lost the person who meant the most to me.      1  \n",
       "4  The time I knocked a deer down - the sight of ...      1  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this, it looks like I'm after 'Field1' (emotion) and SIT (associated text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, some users are problemmatic, so I need to include userID as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7666 entries, 0 to 7665\n",
      "Data columns (total 3 columns):\n",
      "ID        7666 non-null int64\n",
      "Field1    7666 non-null object\n",
      "SIT       7666 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 179.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df = df[['ID','Field1', 'SIT']]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When I was driving home after  several days of hard work, there รก\\nwas a motorist ahead of me who was driving at 50 km/hour and รก\\nrefused, despite his low speeed to let me overtake.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = df['SIT'][2]\n",
    "line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When I was driving home after  several days of hard work, there was a motorist ahead of me who was driving at 50 km/hour and refused, despite his low speeed to let me overtake.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I need to figure out how to strip weird characters out of the text. This has effectively caught\n",
    "# those pesky accented a's. Somehow they got bundled with newline characters. I'm curious why.\n",
    "s = r'.\\n'\n",
    "re.sub(s, '', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This should do the trick.\n",
    "def remove_accented_chars(line):\n",
    "    pattern = r'.\\n'\n",
    "    return re.sub(pattern, '', line)\n",
    "\n",
    "def remove_brackets(line):\n",
    "    pattern1 = r'^\\[ '\n",
    "    pattern2 = r']$'\n",
    "    s = re.sub(pattern1, '', line)\n",
    "    return re.sub(pattern2, '', s)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On days when I feel close to my partner and other friends.  When I feel at peace with myself and also experience a close contact with people whom I regard greatly.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['SIT'] = df['SIT'].apply(remove_accented_chars)\n",
    "df['SIT'] = df['SIT'].apply(remove_brackets)\n",
    "df['SIT'][14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N/A and duplicate answer cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data, special cases need to be handled / replaced. Some lines say things like... [Same as in anger] - which implies the response is the same as given for anger. Occasionally I've come across 'Nothing', 'Not applicable' or 'None' - these rows need to be deleted b/c they're of no use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N/A answers: [ Do not know.] [ No response.] NO RESPONSE. Doesn't apply. Nothing. None. Not applicable. [ Normally I do not feel disgusted.] [ Can not think of anything just now.] [ Can not think of any situation.] [ Can not remember.] Can't think of any. [ Cannot recall any incident when I felt shame.] [ Do not remember any situation of that kind.] [ Can not think of anything.] [ I do not remember when I last felf ashamed. I do not usually feel ashamed of what I do.] Not applicable to myself. Can't remember any episode of disgust. Blank. Not included on questionnaire. Cannot recall the emotion with any force. Haven't been frightened for ages. Haven't felt shame for ages. Cannot remember such a situation. DO NOT REMEMBER. [ No description.] [ I have not felt this emotion.] [ Never felt the emotion.] [ I have not felt this emotion in my life.] NO RESPONSE (w/o the period) [ I have never felt this emotion.] [ I have felt shame but am unable to remember any particular incident.] [ I have not felt this emotion yet.] [ Never experienced.] [ Never] [ There are many instances which are all equally irratating.] [ Honestly, I have never felt disgust at any situation in my life.] [ Sorry, I was never ashamed about anything in my life.] [ I can positively say that I have never done anything that made me feel guilty.] [ I am quite shameless, not applicable.] [ Not applicable.] [ Do not remember any incident.] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate answers: [Same as in anger.], [Same as above ... ], see answer for \"shame\"., The same as in \"shame\"., [ The same as in shame.'], [ The same as in anger.], The same event described under \"shame\"., As in sadness (A), relating to this slaghter of fur-seals., [ The same as in guilt.], The same as in SHAME.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "na_answers = '''Do not know.\n",
    "No response.\n",
    "NO RESPONSE.\n",
    "Doesn't apply.\n",
    "Nothing.\n",
    "None.\n",
    "Not applicable.\n",
    "Normally I do not feel disgusted.\n",
    "Can not think of anything just now.\n",
    "Can not think of any situation.\n",
    "Can not remember.\n",
    "Can't think of anything.\n",
    "I do not remember when I last felf ashamed. I do not usually feel ashamed of what I do.\n",
    "Not applicable to myself.\n",
    "Can't remember any episode of disgust.\n",
    "Blank.\n",
    "Not included on questionnaire.\n",
    "Cannot recall the emotion with any force.\n",
    "Haven't been frightened for ages.\n",
    "Haven't felt shame for ages.\n",
    "Cannot remember such a situation.\n",
    "DO NOT REMEMBER.\n",
    "No description.\n",
    "I have not felt this emotion.\n",
    "Never felt the emotion.\n",
    "I have not felt this emotion in my life.\n",
    "NO RESPONSE\n",
    "I have never felt this emotion.\n",
    "I have felt shame but am unable to remember any particular incident.\n",
    "I have not felt this emotion yet.\n",
    "Never experienced.\n",
    "Never\n",
    "There are many instances which are all equally irratating.\n",
    "Honestly, I have never felt disgust at any situation in my life.\n",
    "Sorry, I was never ashamed of anything in my life.\n",
    "I can positively say that I have never done anything that made me feel guilty.\n",
    "I am quite shameless, not applicable.\n",
    "Do not remember any incident\n",
    "Same as in anger.\n",
    "see answer for \"shame\".\n",
    "The same as in shame.\n",
    "The same as in anger.\n",
    "The same event described under \"shame\".\n",
    "As in sadness (A), relating to this slaughter of fur seals.\n",
    "The same as in guilt.\n",
    "The same as in SHAME.'''\n",
    "na_answers = na_answers.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Writing a function to replace all the non-ansers with np.nan so I can remove them effectively from the dataset.\n",
    "\n",
    "def replace_non_answers(line):\n",
    "    if line in na_answers:\n",
    "        return np.nan\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7666 entries, 0 to 7665\n",
      "Data columns (total 3 columns):\n",
      "ID        7666 non-null int64\n",
      "Field1    7666 non-null object\n",
      "SIT       7530 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 179.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df['SIT'] = df['SIT'].apply(replace_non_answers)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: \n",
    "User 261035 sucks dick. His answers aren't even worth catching. It's easier just to exclude him explicitly. Hey, look, he found a friend. 261058."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "problem_users = [261035, 261058]\n",
    "\n",
    "def remove_problem_users(row):\n",
    "    if row['ID'] in problem_users:\n",
    "        row['SIT'] = np.nan\n",
    "        return row\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7516 entries, 0 to 7665\n",
      "Data columns (total 3 columns):\n",
      "ID        7516 non-null int64\n",
      "Field1    7516 non-null object\n",
      "SIT       7516 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 234.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# let's clip the dataset. It should be ready for processing at this point.\n",
    "df = df.apply(remove_problem_users, axis=1).dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7516 entries, 0 to 7665\n",
      "Data columns (total 2 columns):\n",
      "Field1    7516 non-null object\n",
      "SIT       7516 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 176.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# The ID column isn't a useful feature - it was just helpful for cleaning. let's get rid of it.\n",
    "df = df[['Field1', 'SIT']]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building / a model, it's become clear that there are still issues with the data in that a graph search yields a model with 56.7% accuracy. One of the things I neglected to account for was the ample spelling errors in the dataset. I found a stackoverflow post that might help me out here. https://stackoverflow.com/questions/35070452/how-to-correct-spelling-in-a-pandas-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['SIT'] = df['SIT'].apply(lambda txt: ''.join(TextBlob(txt).correct()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Since there should be a rather large vocabulary, I think implementing a hashing vectorizer might be the correct choice. However, since there are drawbacks (no idf capabilities, no inverse transform capabilities...), I might as well give the tfidf vectorizer a shot first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = df['SIT'], df['Field1']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed: 22.5min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed: 38.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tfidfvectorizer__ngram_range': (1, 2), 'svc__C': 10, 'svc__gamma': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5779670037253859"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = make_pipeline(TfidfVectorizer(), SVC())\n",
    "param_grid = {'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (2, 2)], \n",
    "             'svc__C': [.01, .1, 1, 10, 100],\n",
    "             'svc__gamma': [.01, .1, 1, 10]}\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, cv=10, n_jobs=-1, verbose = 2)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99911300337058717"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Curious to see if maybe we're over-fitting.\n",
    "grid.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It really looks like we might be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after using TextBlob to correct spelling & playing with the random state param, it doesn't seem like for the time being that I'll get much more accurate than 55 - 60 %. I'm not sure if it's just the quality of the dataset that I'm using or the particular models I'm using -- everywhere I read seems to indicate that for a dataset of this size, SVM's are the way to go. I suppose SGD is an option, I've also seen Naive Bayes and Maximum Entropy classifiers used. NB would certainly be a simpler model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Just noting, gridsearchcv found best params to be ngram_range = (1, 2), svc C = 10, svc gamma = 1. However, 57.7% test accuracy and 99.9% train accuracy may indicate overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
